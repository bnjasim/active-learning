{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning on  3 Class Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'activelearn' from '../activelearn.pyc'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp.load_source('activelearn', '../activelearn.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from activelearn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import text\n",
    "\n",
    "class loader(object):\n",
    "    def __init__(self, init_seed, maxlen, nb_words, skip_top, test_split):\n",
    "        self.start_char = 1\n",
    "        self.oov_char = 2\n",
    "        self.index_from = 3\n",
    "        \n",
    "        label_type = '/label.4class.' # '/rating.'\n",
    "        \n",
    "        data_dir = \"datasets/scale_data/scaledata/\"\n",
    "        files = [\"Dennis+Schwartz\", \"James+Berardinelli\", \"Scott+Renshaw\", \"Steve+Rhodes\"]\n",
    "        texts, ratings = [], []\n",
    "        for file in files:\n",
    "            with open(data_dir + file + \"/subj.\" + file, \"r\") as f:\n",
    "                texts += list(f)\n",
    "            with open(data_dir + file + label_type + file, \"r\") as f:\n",
    "                ratings += list(f)\n",
    "        tokenizer = text.Tokenizer(filters='')\n",
    "        tokenizer.fit_on_texts(texts)\n",
    "        X = tokenizer.texts_to_sequences(texts)\n",
    "        Y = [float(rating) for rating in ratings]\n",
    "\n",
    "        # Shuffle data:\n",
    "        np.random.seed(init_seed)\n",
    "        np.random.shuffle(X)\n",
    "        np.random.seed(init_seed)\n",
    "        np.random.shuffle(Y)\n",
    "\n",
    "        # Parse data\n",
    "        X = [[self.start_char] + [w + self.index_from for w in x] for x in X]\n",
    "\n",
    "        new_X = []\n",
    "        new_Y = []\n",
    "        for x, y in zip(X, Y):\n",
    "            for i in xrange(0, len(x), maxlen):\n",
    "                new_X.append(x[i:i+maxlen])\n",
    "                new_Y.append(y)\n",
    "        X = new_X\n",
    "        Y = new_Y\n",
    "\n",
    "        # by convention, use 2 as OOV word\n",
    "        # reserve 'index_from' (=3 by default) characters: 0 (padding), 1 (start), 2 (OOV)\n",
    "        X = [[self.oov_char if (w >= nb_words or w < skip_top) else w for w in x] for x in X]\n",
    "\n",
    "        self.X_train = X[:int(len(X)*(1-test_split))]\n",
    "        self.Y_train = Y[:int(len(X)*(1-test_split))]\n",
    "        self.mean_y_train = np.mean(self.Y_train)\n",
    "        self.std_y_train = np.std(self.Y_train)\n",
    "        #self.Y_train = [(y - self.mean_y_train) / self.std_y_train for y in self.Y_train]\n",
    "\n",
    "        self.X_test = X[int(len(X)*(1-test_split)):]\n",
    "        self.Y_test = Y[int(len(X)*(1-test_split)):]\n",
    "\n",
    "        print(len(self.X_train), 'train sequences')\n",
    "        print(len(self.X_test), 'test sequences')\n",
    "\n",
    "        print(\"Pad sequences (samples x time)\")\n",
    "        self.X_train = sequence.pad_sequences(self.X_train, maxlen=maxlen)\n",
    "        self.X_test = sequence.pad_sequences(self.X_test, maxlen=maxlen)\n",
    "        print('X_train shape:', self.X_train.shape)\n",
    "        print('X_test shape:', self.X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_W, p_U, p_dense, p_emb, weight_decay, batch_size, maxlen = [\"0.25\", \"0.25\", \"0.25\", \"0.25\", \"1e-4\", \"128\", \"200\"]\n",
    "batch_size = int(batch_size)\n",
    "maxlen = int(maxlen)\n",
    "#folder = \"/scratch/home/Projects/rnn_dropout/exps/\"\n",
    "\n",
    "# Global params:\n",
    "nb_words = 20000\n",
    "skip_top = 0\n",
    "test_split = 0.2\n",
    "init_seed = 1\n",
    "global_seed = 0\n",
    "\n",
    "num_classes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "(10620, 'train sequences')\n",
      "(2655, 'test sequences')\n",
      "Pad sequences (samples x time)\n",
      "('X_train shape:', (10620, 200))\n",
      "('X_test shape:', (2655, 200))\n"
     ]
    }
   ],
   "source": [
    "# Load data:\n",
    "print(\"Loading data...\")\n",
    "dataset = loader(init_seed, maxlen, nb_words, skip_top, test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = dataset.X_train, dataset.X_test, dataset.Y_train, dataset.Y_test\n",
    "\n",
    "X_train = np.asarray(X_train)\n",
    "X_test  = np.asarray(X_test)\n",
    "Y_train = np.asarray(Y_train)\n",
    "Y_test  = np.asarray(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1337"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(Y_train == 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, RMSprop, Adagrad\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, SpatialDropout1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_1hot = np.zeros([Y_train.shape[0], num_classes])\n",
    "Y_train_1hot[Y_train==0] = [1,0,0,0]\n",
    "Y_train_1hot[Y_train==1] = [0,1,0,0]\n",
    "Y_train_1hot[Y_train==2] = [0,0,1,0]\n",
    "Y_train_1hot[Y_train==3] = [0,0,0,1]\n",
    "\n",
    "\n",
    "Y_test_1hot = np.zeros([Y_test.shape[0], num_classes])\n",
    "Y_test_1hot[Y_test==0] = [1,0,0,0]\n",
    "Y_test_1hot[Y_test==1] = [0,1,0,0]\n",
    "Y_test_1hot[Y_test==2] = [0,0,1,0]\n",
    "Y_test_1hot[Y_test==3] = [0,0,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "# Build model:\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words + dataset.index_from, 100, embeddings_regularizer=l2(weight_decay), \n",
    "                    input_length=maxlen)) # batch_input_shape=(batch_size, maxlen)\n",
    "\n",
    "model.add(SpatialDropout1D(p_emb))\n",
    "\n",
    "#model.add(LSTM(128, W_regularizer=l2(weight_decay), U_regularizer=l2(weight_decay),\n",
    "#               b_regularizer=l2(weight_decay), dropout_W=p_W, dropout_U=p_U))\n",
    "\n",
    "model.add(LSTM(128, dropout=p_U, recurrent_dropout=p_W))\n",
    "\n",
    "model.add(Dropout(p_dense))\n",
    "#model.add(Dense(4, W_regularizer=l2(weight_decay), b_regularizer=l2(weight_decay)))\n",
    "model.add(Dense(4, activation='sigmoid'))\n",
    "\n",
    "#optimiser = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=False)\n",
    "optimiser = 'adam'\n",
    "model.compile(loss='mean_squared_error', optimizer=optimiser)\n",
    "\n",
    "# Potentially load weights\n",
    "# model.load_weights(\"path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 200, 100)          2000300   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_5 (Spatial (None, 200, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 2,118,064\n",
      "Trainable params: 2,118,064\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "10496/10496 [==============================] - 35s 3ms/step - loss: 0.2168\n",
      "Epoch 2/15\n",
      "10496/10496 [==============================] - 35s 3ms/step - loss: 0.1774\n",
      "Epoch 3/15\n",
      "10496/10496 [==============================] - 34s 3ms/step - loss: 0.1769\n",
      "Epoch 4/15\n",
      "10496/10496 [==============================] - 33s 3ms/step - loss: 0.1767\n",
      "Epoch 5/15\n",
      "10496/10496 [==============================] - 34s 3ms/step - loss: 0.1765\n",
      "Epoch 6/15\n",
      "10496/10496 [==============================] - 34s 3ms/step - loss: 0.1761\n",
      "Epoch 7/15\n",
      "10496/10496 [==============================] - 34s 3ms/step - loss: 0.1738\n",
      "Epoch 8/15\n",
      "10496/10496 [==============================] - 34s 3ms/step - loss: 0.1699\n",
      "Epoch 9/15\n",
      "10496/10496 [==============================] - 34s 3ms/step - loss: 0.1653\n",
      "Epoch 10/15\n",
      "10496/10496 [==============================] - 34s 3ms/step - loss: 0.1574\n",
      "Epoch 11/15\n",
      "10496/10496 [==============================] - 34s 3ms/step - loss: 0.1591\n",
      "Epoch 12/15\n",
      "10496/10496 [==============================] - 33s 3ms/step - loss: 0.1471\n",
      "Epoch 13/15\n",
      "10496/10496 [==============================] - 34s 3ms/step - loss: 0.1346\n",
      "Epoch 14/15\n",
      "10496/10496 [==============================] - 34s 3ms/step - loss: 0.1212\n",
      "Epoch 15/15\n",
      "10496/10496 [==============================] - 34s 3ms/step - loss: 0.1124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd9a8293050>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorflow_train_size = batch_size * (len(X_train) / batch_size)\n",
    "model.fit(X_train[:tensorflow_train_size], np.array(Y_train_1hot[:tensorflow_train_size]),\n",
    "           batch_size=batch_size, epochs=15)#, callbacks=[modeltest_1, modeltest_2])\n",
    "\n",
    "# Potentially save weights\n",
    "# model.save_weights(\"path\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10620/10620 [==============================] - 6s 538us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.83502824858757063"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model\n",
    "# Dropout approximation for training data:\n",
    "train_prob = model.predict(X_train, batch_size=128, verbose=1)\n",
    "np.sum(np.argmax(train_prob, axis=1) == Y_train)*1.0/Y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2655/2655 [==============================] - 0s 153us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.43427495291902074"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropout approximation for test data:\n",
    "test_prob = model.predict(X_test, batch_size=500, verbose=1)\n",
    "np.sum(np.argmax(test_prob, axis=1) == Y_test)*1.0/Y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras IMBD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "(25000, 'train sequences')\n",
      "(25000, 'test sequences')\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = x_train[:500]\n",
    "Y_train = y_train[:500]\n",
    "\n",
    "X_test = x_test[:5000]\n",
    "Y_test = y_test[:5000]\n",
    "\n",
    "# Pool is the unsupervised dataset\n",
    "x_pool = x_train[10000:20000]\n",
    "y_pool = y_train[10000:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "('x_train shape:', (500, 80))\n",
      "('x_test shape:', (5000, 80))\n",
      "('x_pool shape:', (10000, 80))\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('x_train shape:', X_train.shape)\n",
    "print('x_test shape:', X_test.shape)\n",
    "\n",
    "X_pool = sequence.pad_sequences(x_pool, maxlen=maxlen)\n",
    "Y_pool = y_pool\n",
    "print('x_pool shape:', X_pool.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 500 samples, validate on 5000 samples\n",
      "Epoch 1/3\n",
      "500/500 [==============================] - 10s 21ms/step - loss: 0.6931 - acc: 0.4860 - val_loss: 0.6911 - val_acc: 0.5570\n",
      "Epoch 2/3\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.6703 - acc: 0.7440 - val_loss: 0.6613 - val_acc: 0.6110\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.5603 - acc: 0.8500 - val_loss: 0.6408 - val_acc: 0.6368\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd958d184d0>"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=3,\n",
    "          validation_data=(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 5s 1ms/step\n",
      "('Test score:', 0.49929289445877073)\n",
      "('Test accuracy:', 0.80640000000000001)\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 9s 2ms/step\n",
      "('Test score:', 1.008508877468109)\n",
      "('Test accuracy:', 0.6714)\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Learning Procedure\n",
    "1. Allocate an unsupervised set and a test set\n",
    "2. Initially pick some (say 100) from the unsupervised set, get the labels and set it as the train set\n",
    "3. Iterate: pick 10 (vary this parameter) most promising from the unsupervised and add to the train set - retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, imp\n",
    "imp.load_source('activelearn', '../activelearn.py')\n",
    "from activelearn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x_pool shape:', (10000, 80))\n"
     ]
    }
   ],
   "source": [
    "# Pool is the unsupervised dataset\n",
    "x_pool = x_train[10000:20000]\n",
    "y_pool = y_train[10000:20000]\n",
    "\n",
    "X_pool = sequence.pad_sequences(x_pool, maxlen=maxlen)\n",
    "Y_pool = y_pool\n",
    "print('x_pool shape:', X_pool.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25020,)"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_pick(pool_data, pool_labels, num):\n",
    "    '''Pick num number of datapoints from the unsupervised data pool\n",
    "    Remove them from the pool and return the data.\n",
    "    Returns chosen datapoints and the updated pool_data'''\n",
    "    if len(pool_data) < num:\n",
    "        raise Exception('pool data is too small')\n",
    "        \n",
    "    #np.random.seed(0)\n",
    "    indices = np.random.choice(range(len(pool_data)), num, replace=False)\n",
    "    datapoints = pool_data[indices]\n",
    "    labels = pool_labels[indices]\n",
    "    pool_data = np.delete(pool_data, indices, axis=0)\n",
    "    pool_labels = np.delete(pool_labels, indices, axis=0)\n",
    "    print(\"Picked \" + str(num) + \" datapoints\\nSize of updated unsupervised pool = \" + str(len(pool_data)) + \"\\n\")\n",
    "    return datapoints, labels, pool_data, pool_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picked 100 datapoints\n",
      "Size of updated unsupervised pool = 9900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initially\n",
    "X_train, Y_train, X_pool, Y_pool = init_pick(X_pool, Y_pool, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9900, 80)"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n",
      "Epoch 1/3\n",
      "100/100 [==============================] - 5s 50ms/step - loss: 0.6939 - acc: 0.4700\n",
      "Epoch 2/3\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.6856 - acc: 0.7800\n",
      "Epoch 3/3\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6744 - acc: 0.9300\n",
      "5000/5000 [==============================] - 7s 1ms/step\n",
      "('Test accuracy:', 0.53300000000000003)\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "# Initial training\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=3) #, validation_data=(X_test, Y_test))\n",
    "\n",
    "score, acc = model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "#print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "results[X_train.shape[0]] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{100: 0.53300000000000003}"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_pick(acq_fn, num_samples, pool_data, pool_labels, pool_subset_count = 2000):\n",
    "    \"\"\"Inputs: Unsupervised data, an acquisition function and number of samples to return\n",
    "    Output: The datapoints from unsupervised data which has the highest value as per the acquisition function\n",
    "    \"\"\"\n",
    "    #unsup_data = np.array(unsup_data)\n",
    "    if len(pool_data) < num_samples:\n",
    "        raise Exception('pool data is exhausted')\n",
    "        \n",
    "    if pool_subset_count > len(pool_data):\n",
    "        pool_subset_count = len(pool_data)\n",
    "\n",
    "    #values = [acq_fn(x) for x in pool_data]\n",
    "    pool_subset_random_index = np.random.choice(range(len(pool_data)), pool_subset_count, replace=False)\n",
    "    X_pool_subset = pool_data[pool_subset_random_index]\n",
    "    y_pool_subset = pool_labels[pool_subset_random_index]\n",
    "\n",
    "    print('Search over Pool of Unlabeled Data')\n",
    "\n",
    "    values = acq_fn(X_pool_subset)\n",
    "    pos = np.argpartition(values, -num_samples)[-num_samples:]\n",
    "    datapoints = X_pool_subset[pos]\n",
    "    labels = y_pool_subset[pos]\n",
    "    #print pool_subset_random_index[:10]\n",
    "    pool_data = np.delete(pool_data, (pool_subset_random_index[pos]), axis=0)\n",
    "    pool_labels = np.delete(pool_labels, (pool_subset_random_index[pos]), axis=0)\n",
    "    print(\"Picked \" + str(num_samples) + \" datapoints\\nSize of updated Unsupervised pool = \" + str(len(pool_data)))\n",
    "\n",
    "    return datapoints, labels, pool_data, pool_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_ratio(pool_data):\n",
    "    # Var ratio active learning acquisition function\n",
    "    # D_probs - Deterministic probs as opposed to MC sampling\n",
    "    D_probs = model.predict_proba(pool_data)  \n",
    "    return 1.0 - np.max(D_probs, axis=1)\n",
    "\n",
    "def random_acq(pool_data):\n",
    "    return np.random.rand(len(pool_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1516319308.311072"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timeit\n",
    "timeit.time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ACQUISITION ITERATION 1 of 100\n",
      "Search over Pool of Unlabeled Data\n",
      "Picked 10 datapoints\n",
      "Size of updated Unsupervised pool = 9890\n",
      "Acquired Points added to the training set\n",
      "Train Data size: (110, 80)\n",
      "Unlabeled Pool size: (9890, 80)\n",
      "Train Again with the added points\n",
      "Epoch 1/3\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.6490 - acc: 0.8818\n",
      "Epoch 2/3\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.5536 - acc: 0.8727\n",
      "Epoch 3/3\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.3893 - acc: 0.9455\n",
      "5000/5000 [==============================] - 5s 997us/step\n",
      "('Test accuracy:', 0.58040000000000003)\n",
      "\n",
      "\n",
      "ACQUISITION ITERATION 2 of 100\n",
      "Search over Pool of Unlabeled Data\n",
      "Picked 10 datapoints\n",
      "Size of updated Unsupervised pool = 9880\n",
      "Acquired Points added to the training set\n",
      "Train Data size: (120, 80)\n",
      "Unlabeled Pool size: (9880, 80)\n",
      "Train Again with the added points\n",
      "Epoch 1/3\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 0.4244 - acc: 0.8500\n",
      "Epoch 2/3\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 0.2346 - acc: 0.9500\n",
      "Epoch 3/3\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 0.2595 - acc: 0.9333\n",
      "5000/5000 [==============================] - 5s 1ms/step\n",
      "('Test accuracy:', 0.58660000000000001)\n",
      "\n",
      "\n",
      "ACQUISITION ITERATION 3 of 100\n",
      "Search over Pool of Unlabeled Data\n",
      "Picked 10 datapoints\n",
      "Size of updated Unsupervised pool = 9870\n",
      "Acquired Points added to the training set\n",
      "Train Data size: (130, 80)\n",
      "Unlabeled Pool size: (9870, 80)\n",
      "Train Again with the added points\n",
      "Epoch 1/3\n",
      "130/130 [==============================] - 1s 7ms/step - loss: 0.1986 - acc: 0.9462\n",
      "Epoch 2/3\n",
      "130/130 [==============================] - 1s 6ms/step - loss: 0.1848 - acc: 0.9462\n",
      "Epoch 3/3\n",
      "130/130 [==============================] - 1s 7ms/step - loss: 0.1206 - acc: 0.9769\n",
      "5000/5000 [==============================] - 5s 1ms/step\n",
      "('Test accuracy:', 0.58440000000000003)\n",
      "\n",
      "\n",
      "ACQUISITION ITERATION 4 of 100\n",
      "Search over Pool of Unlabeled Data\n",
      "Picked 10 datapoints\n",
      "Size of updated Unsupervised pool = 9860\n",
      "Acquired Points added to the training set\n",
      "Train Data size: (140, 80)\n",
      "Unlabeled Pool size: (9860, 80)\n",
      "Train Again with the added points\n",
      "Epoch 1/3\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2300 - acc: 0.9357\n",
      "Epoch 2/3\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.1262 - acc: 0.9643\n",
      "Epoch 3/3\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.1990 - acc: 0.9571\n",
      "5000/5000 [==============================] - 5s 997us/step\n",
      "('Test accuracy:', 0.55259999999999998)\n",
      "\n",
      "\n",
      "ACQUISITION ITERATION 5 of 100\n",
      "Search over Pool of Unlabeled Data\n",
      "Picked 10 datapoints\n",
      "Size of updated Unsupervised pool = 9850\n",
      "Acquired Points added to the training set\n",
      "Train Data size: (150, 80)\n",
      "Unlabeled Pool size: (9850, 80)\n",
      "Train Again with the added points\n",
      "Epoch 1/3\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.1536 - acc: 0.9467\n",
      "Epoch 2/3\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.0766 - acc: 0.9800\n",
      "Epoch 3/3\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.0820 - acc: 0.9800\n",
      "5000/5000 [==============================] - 5s 1ms/step\n",
      "('Test accuracy:', 0.60540000000000005)\n",
      "\n",
      "\n",
      "ACQUISITION ITERATION 6 of 100\n",
      "Search over Pool of Unlabeled Data\n",
      "Picked 10 datapoints\n",
      "Size of updated Unsupervised pool = 9840\n",
      "Acquired Points added to the training set\n",
      "Train Data size: (160, 80)\n",
      "Unlabeled Pool size: (9840, 80)\n",
      "Train Again with the added points\n",
      "Epoch 1/3\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.1575 - acc: 0.9625\n",
      "Epoch 2/3\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.1819 - acc: 0.9625\n",
      "Epoch 3/3\n",
      "160/160 [==============================] - 1s 6ms/step - loss: 0.1117 - acc: 0.9813\n",
      "5000/5000 [==============================] - 5s 1ms/step\n",
      "('Test accuracy:', 0.65059999999999996)\n",
      "\n",
      "\n",
      "ACQUISITION ITERATION 7 of 100\n",
      "Search over Pool of Unlabeled Data\n",
      "Picked 10 datapoints\n",
      "Size of updated Unsupervised pool = 9830\n",
      "Acquired Points added to the training set\n",
      "Train Data size: (170, 80)\n",
      "Unlabeled Pool size: (9830, 80)\n",
      "Train Again with the added points\n",
      "Epoch 1/3\n",
      "170/170 [==============================] - 1s 6ms/step - loss: 0.1708 - acc: 0.9529\n",
      "Epoch 2/3\n",
      "170/170 [==============================] - 1s 6ms/step - loss: 0.0656 - acc: 0.9882\n",
      "Epoch 3/3\n",
      "170/170 [==============================] - 1s 6ms/step - loss: 0.0567 - acc: 0.9882\n",
      "5000/5000 [==============================] - 5s 1ms/step\n",
      "('Test accuracy:', 0.60680000000000001)\n",
      "\n",
      "\n",
      "ACQUISITION ITERATION 8 of 100\n",
      "Search over Pool of Unlabeled Data\n",
      "Picked 10 datapoints\n",
      "Size of updated Unsupervised pool = 9820\n",
      "Acquired Points added to the training set\n",
      "Train Data size: (180, 80)\n",
      "Unlabeled Pool size: (9820, 80)\n",
      "Train Again with the added points\n",
      "Epoch 1/3\n",
      "180/180 [==============================] - 1s 6ms/step - loss: 0.2583 - acc: 0.9444\n",
      "Epoch 2/3\n",
      "180/180 [==============================] - 1s 6ms/step - loss: 0.1495 - acc: 0.9667\n",
      "Epoch 3/3\n",
      "180/180 [==============================] - 1s 6ms/step - loss: 0.0893 - acc: 0.9889\n",
      "5000/5000 [==============================] - 5s 1ms/step\n",
      "('Test accuracy:', 0.65600000000000003)\n",
      "\n",
      "\n",
      "ACQUISITION ITERATION 9 of 100\n",
      "Search over Pool of Unlabeled Data\n",
      "Picked 10 datapoints\n",
      "Size of updated Unsupervised pool = 9810\n",
      "Acquired Points added to the training set\n",
      "Train Data size: (190, 80)\n",
      "Unlabeled Pool size: (9810, 80)\n",
      "Train Again with the added points\n",
      "Epoch 1/3\n",
      "190/190 [==============================] - 1s 6ms/step - loss: 0.0823 - acc: 0.9737\n",
      "Epoch 2/3\n",
      "190/190 [==============================] - 1s 5ms/step - loss: 0.0913 - acc: 0.9789\n",
      "Epoch 3/3\n",
      "190/190 [==============================] - 1s 5ms/step - loss: 0.0790 - acc: 0.9842\n",
      "5000/5000 [==============================] - 5s 1ms/step\n",
      "('Test accuracy:', 0.64359999999999995)\n",
      "\n",
      "\n",
      "ACQUISITION ITERATION 10 of 100\n",
      "Search over Pool of Unlabeled Data\n",
      "Picked 10 datapoints\n",
      "Size of updated Unsupervised pool = 9800\n",
      "Acquired Points added to the training set\n",
      "Train Data size: (200, 80)\n",
      "Unlabeled Pool size: (9800, 80)\n",
      "Train Again with the added points\n",
      "Epoch 1/3\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.1042 - acc: 0.9750\n",
      "Epoch 2/3\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0875 - acc: 0.9800\n",
      "Epoch 3/3\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0435 - acc: 0.9950\n",
      "5000/5000 [==============================] - 5s 1ms/step\n",
      "('Test accuracy:', 0.64880000000000004)\n",
      "\n",
      "\n",
      "ACQUISITION ITERATION 11 of 100\n",
      "Search over Pool of Unlabeled Data\n",
      "Picked 10 datapoints\n",
      "Size of updated Unsupervised pool = 9790\n",
      "Acquired Points added to the training set\n",
      "Train Data size: (210, 80)\n",
      "Unlabeled Pool size: (9790, 80)\n",
      "Train Again with the added points\n",
      "Epoch 1/3\n",
      "210/210 [==============================] - 1s 6ms/step - loss: 0.0671 - acc: 0.9810\n",
      "Epoch 2/3\n",
      "210/210 [==============================] - 1s 6ms/step - loss: 0.0355 - acc: 0.9905\n",
      "Epoch 3/3\n",
      "210/210 [==============================] - 1s 6ms/step - loss: 0.0202 - acc: 1.0000\n",
      "5000/5000 [==============================] - 5s 1ms/step\n",
      "('Test accuracy:', 0.64039999999999997)\n",
      "\n",
      "\n",
      "ACQUISITION ITERATION 12 of 100\n",
      "Search over Pool of Unlabeled Data\n",
      "Picked 10 datapoints\n",
      "Size of updated Unsupervised pool = 9780\n",
      "Acquired Points added to the training set\n",
      "Train Data size: (220, 80)\n",
      "Unlabeled Pool size: (9780, 80)\n",
      "Train Again with the added points\n",
      "Epoch 1/3\n",
      "220/220 [==============================] - 1s 5ms/step - loss: 0.0738 - acc: 0.9864\n",
      "Epoch 2/3\n",
      "220/220 [==============================] - 1s 6ms/step - loss: 0.0363 - acc: 0.9909\n",
      "Epoch 3/3\n",
      "220/220 [==============================] - 1s 5ms/step - loss: 0.0194 - acc: 1.0000\n",
      "5000/5000 [==============================] - 5s 1ms/step\n",
      "('Test accuracy:', 0.66900000000000004)\n",
      "\n",
      "\n",
      "ACQUISITION ITERATION 13 of 100\n",
      "Search over Pool of Unlabeled Data\n",
      "Picked 10 datapoints\n",
      "Size of updated Unsupervised pool = 9770\n",
      "Acquired Points added to the training set\n",
      "Train Data size: (230, 80)\n",
      "Unlabeled Pool size: (9770, 80)\n",
      "Train Again with the added points\n",
      "Epoch 1/3\n",
      "230/230 [==============================] - 1s 6ms/step - loss: 0.0962 - acc: 0.9783\n",
      "Epoch 2/3\n",
      "230/230 [==============================] - 1s 6ms/step - loss: 0.0431 - acc: 0.9957\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230/230 [==============================] - 1s 6ms/step - loss: 0.0322 - acc: 0.9913\n",
      "5000/5000 [==============================] - 5s 1ms/step\n",
      "('Test accuracy:', 0.65680000000000005)\n",
      "\n",
      "\n",
      "ACQUISITION ITERATION 14 of 100\n",
      "Search over Pool of Unlabeled Data\n",
      "Picked 10 datapoints\n",
      "Size of updated Unsupervised pool = 9760\n",
      "Acquired Points added to the training set\n",
      "Train Data size: (240, 80)\n",
      "Unlabeled Pool size: (9760, 80)\n",
      "Train Again with the added points\n",
      "Epoch 1/3\n",
      "240/240 [==============================] - 1s 6ms/step - loss: 0.0671 - acc: 0.9708\n",
      "Epoch 2/3\n",
      "240/240 [==============================] - 1s 5ms/step - loss: 0.0155 - acc: 0.9958\n",
      "Epoch 3/3\n",
      "240/240 [==============================] - 1s 6ms/step - loss: 0.0087 - acc: 1.0000\n",
      "5000/5000 [==============================] - 5s 1ms/step\n",
      "('Test accuracy:', 0.65280000000000005)\n",
      "\n",
      "\n",
      "ACQUISITION ITERATION 15 of 100\n",
      "Search over Pool of Unlabeled Data\n",
      "Picked 10 datapoints\n",
      "Size of updated Unsupervised pool = 9750\n",
      "Acquired Points added to the training set\n",
      "Train Data size: (250, 80)\n",
      "Unlabeled Pool size: (9750, 80)\n",
      "Train Again with the added points\n",
      "Epoch 1/3\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.1599 - acc: 0.9640\n",
      "Epoch 2/3\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.1457 - acc: 0.9680\n",
      "Epoch 3/3\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.0711 - acc: 0.9920\n",
      "5000/5000 [==============================] - 5s 1ms/step\n",
      "('Test accuracy:', 0.65739999999999998)\n",
      "\n",
      "\n",
      "ACQUISITION ITERATION 16 of 100\n",
      "Search over Pool of Unlabeled Data\n",
      "Picked 10 datapoints\n",
      "Size of updated Unsupervised pool = 9740\n",
      "Acquired Points added to the training set\n",
      "Train Data size: (260, 80)\n",
      "Unlabeled Pool size: (9740, 80)\n",
      "Train Again with the added points\n",
      "Epoch 1/3\n",
      "260/260 [==============================] - 2s 6ms/step - loss: 0.1014 - acc: 0.9808\n",
      "Epoch 2/3\n",
      "260/260 [==============================] - 1s 6ms/step - loss: 0.0464 - acc: 0.9923\n",
      "Epoch 3/3\n",
      "260/260 [==============================] - 2s 6ms/step - loss: 0.0152 - acc: 0.9962\n",
      "5000/5000 [==============================] - 5s 1ms/step\n",
      "('Test accuracy:', 0.66059999999999997)\n",
      "\n",
      "\n",
      "ACQUISITION ITERATION 17 of 100\n",
      "Search over Pool of Unlabeled Data\n",
      "Picked 10 datapoints\n",
      "Size of updated Unsupervised pool = 9730\n",
      "Acquired Points added to the training set\n",
      "Train Data size: (270, 80)\n",
      "Unlabeled Pool size: (9730, 80)\n",
      "Train Again with the added points\n",
      "Epoch 1/3\n",
      "270/270 [==============================] - 2s 6ms/step - loss: 0.0811 - acc: 0.9852\n",
      "Epoch 2/3\n",
      "270/270 [==============================] - 2s 6ms/step - loss: 0.0204 - acc: 1.0000\n",
      "Epoch 3/3\n",
      "270/270 [==============================] - 2s 6ms/step - loss: 0.0106 - acc: 1.0000\n",
      "5000/5000 [==============================] - 5s 1ms/step\n",
      "('Test accuracy:', 0.63719999999999999)\n",
      "\n",
      "\n",
      "ACQUISITION ITERATION 18 of 100\n",
      "Search over Pool of Unlabeled Data\n",
      "Picked 10 datapoints\n",
      "Size of updated Unsupervised pool = 9720\n",
      "Acquired Points added to the training set\n",
      "Train Data size: (280, 80)\n",
      "Unlabeled Pool size: (9720, 80)\n",
      "Train Again with the added points\n",
      "Epoch 1/3\n",
      "280/280 [==============================] - 1s 5ms/step - loss: 0.0532 - acc: 0.9857\n",
      "Epoch 2/3\n",
      "280/280 [==============================] - 2s 5ms/step - loss: 0.0298 - acc: 0.9964\n",
      "Epoch 3/3\n",
      "280/280 [==============================] - 2s 5ms/step - loss: 0.0164 - acc: 1.0000\n",
      "5000/5000 [==============================] - 5s 995us/step\n",
      "('Test accuracy:', 0.65959999999999996)\n",
      "\n",
      "\n",
      "ACQUISITION ITERATION 19 of 100\n",
      "Search over Pool of Unlabeled Data\n",
      "Picked 10 datapoints\n",
      "Size of updated Unsupervised pool = 9710\n",
      "Acquired Points added to the training set\n",
      "Train Data size: (290, 80)\n",
      "Unlabeled Pool size: (9710, 80)\n",
      "Train Again with the added points\n",
      "Epoch 1/3\n",
      "290/290 [==============================] - 2s 6ms/step - loss: 0.0457 - acc: 0.9897\n",
      "Epoch 2/3\n",
      "290/290 [==============================] - 2s 6ms/step - loss: 0.0098 - acc: 0.9966\n",
      "Epoch 3/3\n",
      "290/290 [==============================] - 2s 6ms/step - loss: 0.0044 - acc: 1.0000\n",
      "2208/5000 [============>.................] - ETA: 2s"
     ]
    }
   ],
   "source": [
    "start = timeit.time.time()\n",
    "acquisition_iterations = 100\n",
    "for i in range(acquisition_iterations):\n",
    "    print('\\n\\nACQUISITION ITERATION ' + str(i+1) + ' of ' + str(acquisition_iterations))\n",
    "    \n",
    "    X_picked, Y_picked, X_pool, Y_pool = active_pick(random_acq, 10, X_pool, Y_pool)\n",
    "    print('Acquired Points added to the training set')\n",
    "    X_train = np.concatenate((X_train, X_picked), axis=0)\n",
    "    Y_train = np.concatenate((Y_train, Y_picked), axis=0)\n",
    "    print('Train Data size: ' + str(X_train.shape))  \n",
    "    print('Unlabeled Pool size: ' + str(X_pool.shape))\n",
    "\n",
    "    print('Train Again with the added points')\n",
    "\n",
    "    model.fit(X_train, Y_train, batch_size=batch_size, epochs=3) #, validation_data=(X_test, Y_test))\n",
    "\n",
    "    score, acc = model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "    #print('Test score:', score)\n",
    "    print('Test accuracy:', acc)\n",
    "    results[X_train.shape[0]] = acc\n",
    "\n",
    "end = timeit.time.time()\n",
    "print('\\n Total time = ' + str(end-start) + 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{100: 0.51419999999999999,\n",
       " 110: 0.59219999999999995,\n",
       " 120: 0.56699999999999995,\n",
       " 130: 0.63319999999999999,\n",
       " 140: 0.61480000000000001,\n",
       " 150: 0.60519999999999996,\n",
       " 160: 0.60499999999999998,\n",
       " 170: 0.58440000000000003,\n",
       " 180: 0.61040000000000005,\n",
       " 190: 0.5988,\n",
       " 200: 0.58760000000000001,\n",
       " 210: 0.65000000000000002,\n",
       " 220: 0.60819999999999996,\n",
       " 230: 0.63560000000000005,\n",
       " 240: 0.61799999999999999,\n",
       " 250: 0.63539999999999996,\n",
       " 260: 0.66279999999999994,\n",
       " 270: 0.65359999999999996,\n",
       " 280: 0.61519999999999997,\n",
       " 290: 0.62980000000000003,\n",
       " 300: 0.64100000000000001,\n",
       " 310: 0.59399999999999997,\n",
       " 320: 0.61599999999999999,\n",
       " 330: 0.61419999999999997,\n",
       " 340: 0.63759999999999994,\n",
       " 350: 0.65859999999999996,\n",
       " 360: 0.6462,\n",
       " 370: 0.66820000000000002,\n",
       " 380: 0.65359999999999996,\n",
       " 390: 0.64959999999999996,\n",
       " 400: 0.66739999999999999,\n",
       " 410: 0.64959999999999996,\n",
       " 420: 0.63719999999999999,\n",
       " 430: 0.63539999999999996,\n",
       " 440: 0.63219999999999998,\n",
       " 450: 0.65480000000000005,\n",
       " 460: 0.60899999999999999,\n",
       " 470: 0.64639999999999997,\n",
       " 480: 0.6482,\n",
       " 490: 0.66800000000000004,\n",
       " 500: 0.63539999999999996,\n",
       " 510: 0.66400000000000003,\n",
       " 520: 0.63219999999999998,\n",
       " 530: 0.62580000000000002,\n",
       " 540: 0.64439999999999997,\n",
       " 550: 0.62560000000000004,\n",
       " 560: 0.61360000000000003,\n",
       " 570: 0.623,\n",
       " 580: 0.62139999999999995,\n",
       " 590: 0.65139999999999998,\n",
       " 600: 0.6502,\n",
       " 610: 0.66000000000000003,\n",
       " 620: 0.63859999999999995,\n",
       " 630: 0.64880000000000004,\n",
       " 640: 0.64180000000000004,\n",
       " 650: 0.64180000000000004,\n",
       " 660: 0.64080000000000004,\n",
       " 670: 0.63819999999999999,\n",
       " 680: 0.64059999999999995,\n",
       " 690: 0.63260000000000005,\n",
       " 700: 0.62760000000000005,\n",
       " 710: 0.627,\n",
       " 720: 0.63980000000000004,\n",
       " 730: 0.64839999999999998,\n",
       " 740: 0.64939999999999998,\n",
       " 750: 0.62819999999999998,\n",
       " 760: 0.624,\n",
       " 770: 0.65100000000000002,\n",
       " 780: 0.63260000000000005,\n",
       " 790: 0.62219999999999998,\n",
       " 800: 0.62860000000000005,\n",
       " 810: 0.63939999999999997,\n",
       " 820: 0.63160000000000005,\n",
       " 830: 0.60880000000000001,\n",
       " 840: 0.60919999999999996,\n",
       " 850: 0.60699999999999998,\n",
       " 860: 0.61599999999999999,\n",
       " 870: 0.61180000000000001,\n",
       " 880: 0.62539999999999996,\n",
       " 890: 0.63380000000000003,\n",
       " 900: 0.63180000000000003,\n",
       " 910: 0.63739999999999997,\n",
       " 920: 0.64419999999999999,\n",
       " 930: 0.64559999999999995,\n",
       " 940: 0.64800000000000002,\n",
       " 950: 0.64319999999999999,\n",
       " 960: 0.64839999999999998,\n",
       " 970: 0.64580000000000004,\n",
       " 980: 0.63700000000000001,\n",
       " 990: 0.63319999999999999,\n",
       " 1000: 0.62919999999999998,\n",
       " 1010: 0.63759999999999994,\n",
       " 1020: 0.64559999999999995,\n",
       " 1030: 0.64039999999999997,\n",
       " 1040: 0.64939999999999998,\n",
       " 1050: 0.625,\n",
       " 1060: 0.62360000000000004,\n",
       " 1070: 0.65900000000000003,\n",
       " 1080: 0.66339999999999999,\n",
       " 1090: 0.65000000000000002,\n",
       " 1100: 0.65000000000000002}"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acq1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "acq1 = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search over Pool of Unlabeled Data\n",
      "Picked 10 datapoints\n",
      "Size of updated Unsupervised pool = 9890\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_picked, Y_picked, X_pool, Y_pool = active_pick(var_ratio, 10, X_pool, Y_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_picked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 80)"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquired Points added to the training set\n",
      "Train Data size: (110, 80)\n",
      "Unlabeled Pool size: (9890, 80)\n",
      "Train Again with the added points\n",
      "Epoch 1/3\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.6530 - acc: 0.6000\n",
      "Epoch 2/3\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.6173 - acc: 0.5818\n",
      "Epoch 3/3\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.5553 - acc: 0.8364\n",
      "5000/5000 [==============================] - 5s 995us/step\n",
      "('Test accuracy:', 0.55100000000000005)\n"
     ]
    }
   ],
   "source": [
    "print('Acquired Points added to the training set')\n",
    "X_train = np.concatenate((X_train, X_picked), axis=0)\n",
    "Y_train = np.concatenate((Y_train, Y_picked), axis=0)\n",
    "print('Train Data size: ' + str(X_train.shape))  \n",
    "print('Unlabeled Pool size: ' + str(X_pool.shape))\n",
    "\n",
    "print('Train Again with the added points')\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=3) #, validation_data=(X_test, Y_test))\n",
    "\n",
    "score, acc = model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "#print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "results[X_train.shape[0]] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python27]",
   "language": "python",
   "name": "conda-env-python27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
