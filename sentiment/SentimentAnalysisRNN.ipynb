{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning on  3 Class Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'activelearn' from '../activelearn.pyc'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp.load_source('activelearn', '../activelearn.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from activelearn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import text\n",
    "\n",
    "class loader(object):\n",
    "    def __init__(self, init_seed, maxlen, nb_words, skip_top, test_split):\n",
    "        self.start_char = 1\n",
    "        self.oov_char = 2\n",
    "        self.index_from = 3\n",
    "        \n",
    "        label_type = '/label.4class.' # '/rating.'\n",
    "        \n",
    "        data_dir = \"datasets/scale_data/scaledata/\"\n",
    "        files = [\"Dennis+Schwartz\", \"James+Berardinelli\", \"Scott+Renshaw\", \"Steve+Rhodes\"]\n",
    "        texts, ratings = [], []\n",
    "        for file in files:\n",
    "            with open(data_dir + file + \"/subj.\" + file, \"r\") as f:\n",
    "                texts += list(f)\n",
    "            with open(data_dir + file + label_type + file, \"r\") as f:\n",
    "                ratings += list(f)\n",
    "        tokenizer = text.Tokenizer(filters='')\n",
    "        tokenizer.fit_on_texts(texts)\n",
    "        X = tokenizer.texts_to_sequences(texts)\n",
    "        Y = [float(rating) for rating in ratings]\n",
    "\n",
    "        # Shuffle data:\n",
    "        np.random.seed(init_seed)\n",
    "        np.random.shuffle(X)\n",
    "        np.random.seed(init_seed)\n",
    "        np.random.shuffle(Y)\n",
    "\n",
    "        # Parse data\n",
    "        X = [[self.start_char] + [w + self.index_from for w in x] for x in X]\n",
    "\n",
    "        new_X = []\n",
    "        new_Y = []\n",
    "        for x, y in zip(X, Y):\n",
    "            for i in xrange(0, len(x), maxlen):\n",
    "                new_X.append(x[i:i+maxlen])\n",
    "                new_Y.append(y)\n",
    "        X = new_X\n",
    "        Y = new_Y\n",
    "\n",
    "        # by convention, use 2 as OOV word\n",
    "        # reserve 'index_from' (=3 by default) characters: 0 (padding), 1 (start), 2 (OOV)\n",
    "        X = [[self.oov_char if (w >= nb_words or w < skip_top) else w for w in x] for x in X]\n",
    "\n",
    "        self.X_train = X[:int(len(X)*(1-test_split))]\n",
    "        self.Y_train = Y[:int(len(X)*(1-test_split))]\n",
    "        self.mean_y_train = np.mean(self.Y_train)\n",
    "        self.std_y_train = np.std(self.Y_train)\n",
    "        #self.Y_train = [(y - self.mean_y_train) / self.std_y_train for y in self.Y_train]\n",
    "\n",
    "        self.X_test = X[int(len(X)*(1-test_split)):]\n",
    "        self.Y_test = Y[int(len(X)*(1-test_split)):]\n",
    "\n",
    "        print(len(self.X_train), 'train sequences')\n",
    "        print(len(self.X_test), 'test sequences')\n",
    "\n",
    "        print(\"Pad sequences (samples x time)\")\n",
    "        self.X_train = sequence.pad_sequences(self.X_train, maxlen=maxlen)\n",
    "        self.X_test = sequence.pad_sequences(self.X_test, maxlen=maxlen)\n",
    "        print('X_train shape:', self.X_train.shape)\n",
    "        print('X_test shape:', self.X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_W, p_U, p_dense, p_emb, weight_decay, batch_size, maxlen = [\"0.25\", \"0.25\", \"0.25\", \"0.25\", \"1e-4\", \"128\", \"200\"]\n",
    "batch_size = int(batch_size)\n",
    "maxlen = int(maxlen)\n",
    "#folder = \"/scratch/home/Projects/rnn_dropout/exps/\"\n",
    "\n",
    "# Global params:\n",
    "nb_words = 20000\n",
    "skip_top = 0\n",
    "test_split = 0.2\n",
    "init_seed = 1\n",
    "global_seed = 0\n",
    "\n",
    "num_classes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "(10620, 'train sequences')\n",
      "(2655, 'test sequences')\n",
      "Pad sequences (samples x time)\n",
      "('X_train shape:', (10620, 200))\n",
      "('X_test shape:', (2655, 200))\n"
     ]
    }
   ],
   "source": [
    "# Load data:\n",
    "print(\"Loading data...\")\n",
    "dataset = loader(init_seed, maxlen, nb_words, skip_top, test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = dataset.X_train, dataset.X_test, dataset.Y_train, dataset.Y_test\n",
    "\n",
    "X_train = np.asarray(X_train)\n",
    "X_test  = np.asarray(X_test)\n",
    "Y_train = np.asarray(Y_train)\n",
    "Y_test  = np.asarray(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1337"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(Y_train == 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, RMSprop, Adagrad\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, SpatialDropout1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_1hot = np.zeros([Y_train.shape[0], num_classes])\n",
    "Y_train_1hot[Y_train==0] = [1,0,0,0]\n",
    "Y_train_1hot[Y_train==1] = [0,1,0,0]\n",
    "Y_train_1hot[Y_train==2] = [0,0,1,0]\n",
    "Y_train_1hot[Y_train==3] = [0,0,0,1]\n",
    "\n",
    "\n",
    "Y_test_1hot = np.zeros([Y_test.shape[0], num_classes])\n",
    "Y_test_1hot[Y_test==0] = [1,0,0,0]\n",
    "Y_test_1hot[Y_test==1] = [0,1,0,0]\n",
    "Y_test_1hot[Y_test==2] = [0,0,1,0]\n",
    "Y_test_1hot[Y_test==3] = [0,0,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "# Build model:\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words + dataset.index_from, 100, embeddings_regularizer=l2(weight_decay), \n",
    "                    input_length=maxlen)) # batch_input_shape=(batch_size, maxlen)\n",
    "\n",
    "model.add(SpatialDropout1D(p_emb))\n",
    "\n",
    "#model.add(LSTM(128, W_regularizer=l2(weight_decay), U_regularizer=l2(weight_decay),\n",
    "#               b_regularizer=l2(weight_decay), dropout_W=p_W, dropout_U=p_U))\n",
    "\n",
    "model.add(LSTM(128, dropout=p_U, recurrent_dropout=p_W))\n",
    "\n",
    "model.add(Dropout(p_dense))\n",
    "#model.add(Dense(4, W_regularizer=l2(weight_decay), b_regularizer=l2(weight_decay)))\n",
    "model.add(Dense(4, activation='sigmoid'))\n",
    "\n",
    "#optimiser = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=False)\n",
    "optimiser = 'adam'\n",
    "model.compile(loss='mean_squared_error', optimizer=optimiser)\n",
    "\n",
    "# Potentially load weights\n",
    "# model.load_weights(\"path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 200, 100)          2000300   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_5 (Spatial (None, 200, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 2,118,064\n",
      "Trainable params: 2,118,064\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "10496/10496 [==============================] - 35s 3ms/step - loss: 0.2168\n",
      "Epoch 2/15\n",
      "10496/10496 [==============================] - 35s 3ms/step - loss: 0.1774\n",
      "Epoch 3/15\n",
      "10496/10496 [==============================] - 34s 3ms/step - loss: 0.1769\n",
      "Epoch 4/15\n",
      "10496/10496 [==============================] - 33s 3ms/step - loss: 0.1767\n",
      "Epoch 5/15\n",
      "10496/10496 [==============================] - 34s 3ms/step - loss: 0.1765\n",
      "Epoch 6/15\n",
      "10496/10496 [==============================] - 34s 3ms/step - loss: 0.1761\n",
      "Epoch 7/15\n",
      "10496/10496 [==============================] - 34s 3ms/step - loss: 0.1738\n",
      "Epoch 8/15\n",
      "10496/10496 [==============================] - 34s 3ms/step - loss: 0.1699\n",
      "Epoch 9/15\n",
      "10496/10496 [==============================] - 34s 3ms/step - loss: 0.1653\n",
      "Epoch 10/15\n",
      "10496/10496 [==============================] - 34s 3ms/step - loss: 0.1574\n",
      "Epoch 11/15\n",
      "10496/10496 [==============================] - 34s 3ms/step - loss: 0.1591\n",
      "Epoch 12/15\n",
      "10496/10496 [==============================] - 33s 3ms/step - loss: 0.1471\n",
      "Epoch 13/15\n",
      "10496/10496 [==============================] - 34s 3ms/step - loss: 0.1346\n",
      "Epoch 14/15\n",
      "10496/10496 [==============================] - 34s 3ms/step - loss: 0.1212\n",
      "Epoch 15/15\n",
      "10496/10496 [==============================] - 34s 3ms/step - loss: 0.1124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd9a8293050>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorflow_train_size = batch_size * (len(X_train) / batch_size)\n",
    "model.fit(X_train[:tensorflow_train_size], np.array(Y_train_1hot[:tensorflow_train_size]),\n",
    "           batch_size=batch_size, epochs=15)#, callbacks=[modeltest_1, modeltest_2])\n",
    "\n",
    "# Potentially save weights\n",
    "# model.save_weights(\"path\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10620/10620 [==============================] - 6s 538us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.83502824858757063"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model\n",
    "# Dropout approximation for training data:\n",
    "train_prob = model.predict(X_train, batch_size=128, verbose=1)\n",
    "np.sum(np.argmax(train_prob, axis=1) == Y_train)*1.0/Y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2655/2655 [==============================] - 0s 153us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.43427495291902074"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropout approximation for test data:\n",
    "test_prob = model.predict(X_test, batch_size=500, verbose=1)\n",
    "np.sum(np.argmax(test_prob, axis=1) == Y_test)*1.0/Y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras IMBD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "(25000, 'train sequences')\n",
      "(25000, 'test sequences')\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = x_train[:500]\n",
    "Y_train = y_train[:500]\n",
    "\n",
    "X_test = x_test[:5000]\n",
    "Y_test = y_test[:5000]\n",
    "\n",
    "# Pool is the unsupervised dataset\n",
    "x_pool = x_train[10000:20000]\n",
    "y_pool = y_train[10000:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "('x_train shape:', (500, 80))\n",
      "('x_test shape:', (5000, 80))\n",
      "('x_pool shape:', (10000, 80))\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('x_train shape:', X_train.shape)\n",
    "print('x_test shape:', X_test.shape)\n",
    "\n",
    "X_pool = sequence.pad_sequences(x_pool, maxlen=maxlen)\n",
    "Y_pool = y_pool\n",
    "print('x_pool shape:', X_pool.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 500 samples, validate on 5000 samples\n",
      "Epoch 1/3\n",
      "500/500 [==============================] - 10s 21ms/step - loss: 0.6931 - acc: 0.4860 - val_loss: 0.6911 - val_acc: 0.5570\n",
      "Epoch 2/3\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.6703 - acc: 0.7440 - val_loss: 0.6613 - val_acc: 0.6110\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.5603 - acc: 0.8500 - val_loss: 0.6408 - val_acc: 0.6368\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd958d184d0>"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=3,\n",
    "          validation_data=(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 5s 1ms/step\n",
      "('Test score:', 0.49929289445877073)\n",
      "('Test accuracy:', 0.80640000000000001)\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 9s 2ms/step\n",
      "('Test score:', 1.008508877468109)\n",
      "('Test accuracy:', 0.6714)\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Learning Procedure\n",
    "1. Allocate an unsupervised set and a test set\n",
    "2. Initially pick some (say 100) from the unsupervised set, get the labels and set it as the train set\n",
    "3. Iterate: pick 10 (vary this parameter) most promising from the unsupervised and add to the train set - retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, imp\n",
    "imp.load_source('activelearn', '../activelearn.py')\n",
    "from activelearn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x_pool shape:', (10000, 80))\n"
     ]
    }
   ],
   "source": [
    "# Pool is the unsupervised dataset\n",
    "x_pool = x_train[10000:20000]\n",
    "y_pool = y_train[10000:20000]\n",
    "\n",
    "X_pool = sequence.pad_sequences(x_pool, maxlen=maxlen)\n",
    "Y_pool = y_pool\n",
    "print('x_pool shape:', X_pool.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_pick(pool_data, pool_labels, num):\n",
    "    '''Pick num number of datapoints from the unsupervised data pool\n",
    "    Remove them from the pool and return the data.\n",
    "    Returns chosen datapoints and the updated pool_data'''\n",
    "    if len(pool_data) < num:\n",
    "        raise Exception('pool data is too small')\n",
    "        \n",
    "    indices = np.random.choice(range(len(pool_data)), num, replace=False)\n",
    "    datapoints = pool_data[indices]\n",
    "    labels = pool_labels[indices]\n",
    "    pool_data = np.delete(pool_data, indices, axis=0)\n",
    "    pool_labels = np.delete(pool_labels, indices, axis=0)\n",
    "    print(\"Picked \" + str(num) + \" datapoints\\nSize of updated unsupervised pool = \" + str(len(pool_data)) + \"\\n\")\n",
    "    return datapoints, labels, pool_data, pool_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picked 100 datapoints\n",
      "Size of updated unsupervised pool = 9800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initially\n",
    "X_train, Y_train, X_pool, Y_pool = init_pick(X_pool, Y_pool, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9800, 80)"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n",
      "Epoch 1/3\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.6915 - acc: 0.6300\n",
      "Epoch 2/3\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6816 - acc: 0.6500\n",
      "Epoch 3/3\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6695 - acc: 0.6500\n",
      "5000/5000 [==============================] - 7s 1ms/step\n",
      "('Test accuracy:', 0.4854)\n"
     ]
    }
   ],
   "source": [
    "# Initial training\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=3) #, validation_data=(X_test, Y_test))\n",
    "\n",
    "score, acc = model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "#print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "results[X_train.shape[0]] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{100: 0.4854}"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_pick(acq_fn, num_samples, pool_data, pool_labels, pool_subset_count = 2000):\n",
    "    \"\"\"Inputs: Unsupervised data, an acquisition function and number of samples to return\n",
    "    Output: The datapoints from unsupervised data which has the highest value as per the acquisition function\n",
    "    \"\"\"\n",
    "    #unsup_data = np.array(unsup_data)\n",
    "    if len(pool_data) < num_samples:\n",
    "        raise Exception('pool data is exhausted')\n",
    "        \n",
    "    if pool_subset_count > len(pool_data):\n",
    "        pool_subset_count = len(pool_data)\n",
    "\n",
    "    #values = [acq_fn(x) for x in pool_data]\n",
    "    pool_subset_random_index = np.random.choice(range(len(pool_data)), pool_subset_count, replace=False)\n",
    "    X_pool_subset = pool_data[pool_subset_random_index]\n",
    "    y_pool_subset = pool_labels[pool_subset_random_index]\n",
    "\n",
    "    print('Search over Pool of Unlabeled Data')\n",
    "\n",
    "    values = acq_fn(X_pool_subset)\n",
    "    pos = np.argpartition(values, -num_samples)[-num_samples:]\n",
    "    datapoints = X_pool_subset[pos]\n",
    "    labels = y_pool_subset[pos]\n",
    "    #print pool_subset_random_index[:10]\n",
    "    pool_data = np.delete(pool_data, (pool_subset_random_index[pos]), axis=0)\n",
    "    pool_labels = np.delete(pool_labels, (pool_subset_random_index[pos]), axis=0)\n",
    "    print(\"Picked \" + str(num_samples) + \" datapoints\\nSize of updated Unsupervised pool = \" + str(len(pool_data)) + \"\\n\")\n",
    "\n",
    "    return datapoints, labels, pool_data, pool_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_ratio(pool_data):\n",
    "    # Var ratio active learning acquisition function\n",
    "    # D_probs - Deterministic probs as opposed to MC sampling\n",
    "    D_probs = model.predict_proba(pool_data)  \n",
    "    return 1.0 - np.max(D_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search over Pool of Unlabeled Data\n",
      "[5739 1624 8132 8577 7438 3925 8814 6392 9244 3774]\n",
      "Picked 10 datapoints\n",
      "Size of updated Unsupervised pool = 9782\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_picked, Y_picked, X_pool, Y_pool = active_pick(var_ratio, 10, X_pool, Y_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_picked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9799, 80)"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Acquired Points added to the training set')\n",
    "X_train = np.concatenate((X_train, acquired_X), axis=0)\n",
    "y_train = np.concatenate((y_train, acquired_Y), axis=0)\n",
    "print('Train Data size: ' + str(X_train.shape))  \n",
    "print('Unlabeled Pool size: ' + str(X_Pool.shape))\n",
    "\n",
    "print('Train Again with the added points')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python27]",
   "language": "python",
   "name": "conda-env-python27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
